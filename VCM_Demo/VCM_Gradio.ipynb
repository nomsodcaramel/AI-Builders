{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Spaces**"
      ],
      "metadata": {
        "id": "DOJZNJ7lDvSD"
      },
      "id": "DOJZNJ7lDvSD"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/spaces/Pipatpong/VCM_Demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey-DVsOlDxeS",
        "outputId": "4414c841-a53a-4039-bef4-fc19ef8b6ade"
      },
      "id": "Ey-DVsOlDxeS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VCM_Demo'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 54 (delta 30), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), 11.87 KiB | 934.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "V6zYJQKuFUAv"
      },
      "id": "V6zYJQKuFUAv",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"Pipatpong/vcm_santa\"\n",
        "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, device_map=\"auto\", load_in_8bit=True)\n",
        "\n",
        "def generate(text, max_length, num_return_sequences=1):\n",
        "    inputs = tokenizer.encode(text, padding=False, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=num_return_sequences)\n",
        "    gen_text = \"Assignment : \" + tokenizer.decode(outputs[0])\n",
        "    if gen_text.count(\"#\") > 2:\n",
        "      split_text = gen_text.split(\"#\", 2)\n",
        "      return split_text[0] + \"#\" + split_text[1]\n",
        "    else:\n",
        "      return gen_text\n",
        "\n",
        "def extract_functions(text):\n",
        "    function_pattern = r'def\\s+(\\w+)\\((.*?)\\):([\\s\\S]*?)return\\s+(.*?)\\n'\n",
        "    functions = re.findall(function_pattern, text, flags=re.MULTILINE)\n",
        "    extracted_text = []\n",
        "\n",
        "    for function in functions:\n",
        "        function_name = function[0]\n",
        "        parameters = function[1]\n",
        "        function_body = function[2]\n",
        "        return_statement = function[3]\n",
        "\n",
        "        extracted_function = f\"def {function_name}({parameters}):\\n    # Code Here\\n    return {return_statement}\\n\"\n",
        "        extracted_text.append(extracted_function)\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "def assignment(text, max_length):\n",
        "    extracted_functions = extract_functions(generate(text, max_length))\n",
        "    for function in extracted_functions:\n",
        "        return function\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        inputs=[gr.inputs.Textbox(placeholder=\"Type here and click the button for the desired action.\", label=\"Prompt\"),\n",
        "                gr.Slider(30, 150, step=10, label=\"Max_length\"),\n",
        "              ]\n",
        "      outputs=gr.outputs.Textbox(label=\"Generated Text\")\n",
        "\n",
        "    with gr.Row():\n",
        "      b1 = gr.Button(\"Assignment\")\n",
        "      b2 = gr.Button(\"Example Code\")\n",
        "\n",
        "      b1.click(assignment, inputs, outputs)\n",
        "      b2.click(generate, inputs, outputs)\n",
        "\n",
        "    examples = [\n",
        "    [\"generate a python for sum number\"],\n",
        "    [\"generate a python function to find max min element of list\"],\n",
        "    [\"generate a python function to find minimum of two numbers with test case\"],\n",
        "    ]\n",
        "\n",
        "    gr.Examples(examples=examples, inputs=inputs, cache_examples=False)\n",
        "\n",
        "demo.launch(share=False, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "l65qlQY4EH_u",
        "outputId": "e7f405e9-bf8a-4446-9d8e-8451928a9f14"
      },
      "id": "l65qlQY4EH_u",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are loading your model in 8bit or 4bit but no linear modules were found in your model. this can happen for some architectures such as gpt2 that uses Conv1D instead of Linear layers. Please double check your model architecture, or submit an issue on github if you think this is a bug.\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:30: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  super().__init__(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:30: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  super().__init__(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqHQ5aoPBUQ_"
      },
      "id": "kqHQ5aoPBUQ_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}